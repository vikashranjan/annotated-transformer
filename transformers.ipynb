{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "┌ Info: Precompiling StatsBase [2913bbd2-ae8a-5f71-8c99-4fb6c76f3a91]\n",
      "└ @ Base loading.jl:1278\n"
     ]
    }
   ],
   "source": [
    "using Flux,LinearAlgebra,Distances,StatsBase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "function clone(m::Any, N::Int)\n",
    "    return ([deepcopy(m) for i in 1:N])\n",
    "end\n",
    "\n",
    "mutable struct MultiHeadedAttention\n",
    "        d_model::Int\n",
    "        h::Int\n",
    "        d_k::Int\n",
    "        linears::Array\n",
    "        dropout::Dropout\n",
    "        attn::Union{Array,Nothing}\n",
    "        MultiHeadedAttention(d_model::Int,h::Int,dropout::Float64) = new(\n",
    "        d_model,h,Int(d_model/h),clone(Dense(d_model, d_model), 4),Dropout(dropout),nothing\n",
    "    )\n",
    "\n",
    "end\n",
    "function (mh::MultiHeadedAttention)( query::Array, key::Array, value::Array, mask::Union{Array,Nothing})\n",
    "    \n",
    "    # Same mask applied to all h heads.\n",
    "    if mask != nothing\n",
    "        mask=unsqueeze(mask,2)\n",
    "    end\n",
    "    \n",
    "     nbatches = size(query)[1]\n",
    "    \n",
    "    # 1) Do all the linear projections in batch from d_model => h x d_k \n",
    "     query, key, value = [permutedims(reshape(l(x),nbatches, :, mh.h, mh.d_k),(1,3,2,4))\n",
    "        for (l, x) in zip(mh.linears, (query, key, value))]\n",
    "    \n",
    "    # 2) Apply attention on all the projected vectors in batch. \n",
    "    x, mh.attn = attention(query, key, value, mask, mh.dropout)\n",
    "    \n",
    "    # 3) \"Concat\" using a view and apply a final linear. \n",
    "    x = reshape(permutedims(x,(1,3,2,4)) ,nbatches, :, mh.h * mh.d_k)\n",
    "    \n",
    "    return mh.linears[end](x)\n",
    "end\n",
    "\n",
    "mutable struct PositionwiseFeedForward\n",
    "    w_1::Dense\n",
    "    w_2::Dense\n",
    "    dropout::Dropout\n",
    "    \n",
    "    PositionwiseFeedForward(d_model::Int,d_ff::Int,dropout::Float64) = new(Dense(d_model,d_ff),Dense(d_ff,d_model),Dropout(dropout))\n",
    "\n",
    "end\n",
    "\n",
    "function (pl::PositionwiseFeedForward)(x::Array)\n",
    "    return pl.w_2( pl.dropout( relu(pl.w_1(x) )))\n",
    "end\n",
    "\n",
    "mutable struct Sublayer \n",
    "    size::Int\n",
    "    norm::LayerNorm\n",
    "    dropout::Dropout\n",
    "    Sublayer(size::Int,dropout::Float64) = new(size,LayerNorm(size),Dropout(dropout))\n",
    "end\n",
    "\n",
    "function (sb::Sublayer)(x::Matrix,f::Function)\n",
    "   return x .+ dropout(f(norm(x)))\n",
    "end\n",
    "\n",
    "mutable struct EncoderLayer \n",
    "    size::Int\n",
    "    self_attn::MultiHeadedAttention\n",
    "    feed_forward::PositionwiseFeedForward\n",
    "    sublayer::Array\n",
    "    EncoderLayer(size::Int,\n",
    "        self_attn::MultiHeadedAttention,\n",
    "        feed_forward::PositionwiseFeedForward,\n",
    "        dropout::Float64) = new(size,self_attn,feed_forward,clone(Sublayer(size,dropout),2))\n",
    "end\n",
    "\n",
    "function (el::EncoderLayer)(x::Matrix,src_mask::Matrix)\n",
    "    x = el.sublayer[1](x, (x,src_mask)->el.self_attn(x, x, x, src_mask))\n",
    "    return el.sublayer[2](x, el.feed_forward)\n",
    "end\n",
    "\n",
    "mutable struct Encoder \n",
    "    layers::Array\n",
    "    norm::LayerNorm\n",
    "    Encoder(layer::EncoderLayer,N::Int) = new(clone(layer,N),LayerNorm(layer.size))     \n",
    "end\n",
    "\n",
    "function (en::Encoder)(x::Matrix,src_mask::Matrix)\n",
    "      for layer in en.layers\n",
    "            x = layer(x, src_mask)\n",
    "      end\n",
    "      return self.norm(x)\n",
    "end\n",
    "\n",
    "mutable struct DecoderLayer\n",
    "    size::Int\n",
    "    self_attn::MultiHeadedAttention\n",
    "    src_attn::MultiHeadedAttention\n",
    "    feed_forward::PositionwiseFeedForward\n",
    "    sublayer::Array\n",
    "    \n",
    "    DecoderLayer(size::Int,\n",
    "        self_attn::MultiHeadedAttention,\n",
    "        src_attn::MultiHeadedAttention,\n",
    "        feed_forward::PositionwiseFeedForward,\n",
    "        dropout::Float64) = new(size,self_attn,src_attn,feed_forward,clone(Sublayer(size,dropout),3))\n",
    "end\n",
    "\n",
    "function (dl::DecoderLayer)(x::Matrix,memory::Matrix,src_mask::Matrix,tgt_mask::Matrix)\n",
    "    x = dl.sublayer[1](x, (x,tgt_mask)->dl.self_attn(x, x, x, tgt_mask))\n",
    "    x = dl.sublayer[2](x, (x,src_mask,memory)->dl.src_attn(x, memory, memory, src_mask))\n",
    "    return dl.sublayer[3](x, el.feed_forward)\n",
    "end\n",
    "\n",
    "\n",
    "mutable struct Decoder \n",
    "    layers::Array\n",
    "    norm::LayerNorm\n",
    "    Decoder(layer::DecoderLayer,N::Int) = new(clone(layer,N),LayerNorm(layer.size))  \n",
    "end\n",
    "\n",
    "function (dc::Decoder)(x::Matrix,memory::Matrix,src_mask::Matrix,tgt_mask::Matrix)\n",
    "      for layer in dc.layers\n",
    "            x = layer(x, memory,src_mask,tgt_mask)\n",
    "      end\n",
    "      return self.norm(x)\n",
    "end\n",
    "\n",
    "\n",
    "function subsequent_mask(size::Int)\n",
    "    \"Mask out subsequent positions.\"\n",
    "    return  unsqueeze(Array(LowerTriangular(ones(size,size)).==1.0),1)\n",
    "end\n",
    "\n",
    "function fill_mask!(a::Array,mask::Array,esp=-1e9)\n",
    "    a[.~mask].=esp\n",
    "    return a\n",
    "end\n",
    "\n",
    "function swap_last_2_dimesions!(a::Array)\n",
    "    #swap last 2 dimensions\n",
    "    dimesions=Array(1:length(size(a)))\n",
    "    swp_d_1=dimesions[end]\n",
    "    swp_d_2=dimesions[end-1]\n",
    "    dimesions=append!(dimesions[1:end-2],[swp_d_1,swp_d_2])\n",
    "    return permutedims(a,dimesions)\n",
    "end\n",
    "\n",
    "mutable struct SourceEmbedding\n",
    "    m::Chain\n",
    "end\n",
    "function (se::SourceEmbedding)(x::Array)\n",
    "    return m(x)\n",
    "end\n",
    "\n",
    "mutable struct TargetEmbedding\n",
    "    m::Chain\n",
    "end\n",
    "\n",
    "function (se::TargetEmbedding)(x::Array)\n",
    "    return m(x)\n",
    "end\n",
    "\n",
    "struct Generator\n",
    "    m::Chain    \n",
    "    Generator(d_model::Int,vocab::Int)=new(Chain(Dense(d_model,d_model),logsoftmax))\n",
    "end\n",
    "\n",
    "function (g::Generator)(x::Array)\n",
    "    return g(x)\n",
    "end\n",
    "\n",
    "mutable struct EncoderDecoder\n",
    "    encoder::Encoder\n",
    "    decoder::Decoder\n",
    "    src_embed::SourceEmbedding\n",
    "    tgt_embed::TargetEmbedding\n",
    "    generator::Generator\n",
    "    \n",
    "    EncoderDecoder(\n",
    "    encoder::Encoder,\n",
    "    decoder::Decoder,\n",
    "    src_embed::SourceEmbedding,\n",
    "    tgt_embed::TargetEmbedding,\n",
    "    generator::Generator) =  new(encoder,decoder,src_embed,tgt_embed,generator)\n",
    "end\n",
    "\n",
    "function encode(ed::EncoderDecoder,src::Matrix,src_mask::Matrix)\n",
    "    ed.encoder(ed.src_embed(src,src_mask),src_mask)\n",
    "end\n",
    "\n",
    "function decode(ed::EncoderDecoder,memory::Matrix,src_mask::Matrix,tgt::Matrix,tgt_mask::Matrix)\n",
    "    return ed.decoder(ed.tgt_embed(tgt),memory,src_mask,tgt_mask)\n",
    "end\n",
    "\n",
    "\n",
    "function (ed::EncoderDecoder)(src::Matrix, tgt::Matrix, src_mask::Matrix, tgt_mask::Matrix)\n",
    "    return decode(ed,encode(ed,src,src_mask),src_mask,tgt, tgt_mask)\n",
    "end\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "function attention(query, key, value, mask::Union{Array,Nothing}, dropout::Union{Array,Nothing})\n",
    "    \"Compute 'Scaled Dot Product Attention'\"\n",
    "    d_k = size(query)[end]\n",
    "    scores = (query * swap_last_2_dimesions!(key))./math.sqrt(d_k)\n",
    "    if mask != nothing\n",
    "        scores = masked_fill!(scores,mask)\n",
    "    end\n",
    "    p_attn = softmax(scores, dim = length(size(scores)))\n",
    "    \n",
    "    if dropout != nothing\n",
    "        p_attn = dropout(p_attn)\n",
    "    end\n",
    "    return p_attn*value, p_attn\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#unsqueeze\n",
    "unsqueeze(xs, dim) = reshape(xs, (size(xs)[1:dim-1]..., 1, size(xs)[dim:end]...))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mutable struct EmbeddingLayer\n",
    "   W\n",
    "   EmbeddingLayer(mf, vs) = new(Flux.glorot_normal(mf, vs))\n",
    "end\n",
    "\n",
    "(m::EmbeddingLayer)(x) = m.W * Flux.onehotbatch(reshape(x, pad_size*N), 0:vocab_size-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mutable struct Embeddings\n",
    "\n",
    "        lut::EmbeddingLayer\n",
    "        d_model::Int\n",
    "\n",
    "    Embeddings(d_model::Int, vocab::Int)=new(EmbeddingLayer(d_model,vocab),d_model)\n",
    "    \n",
    "end\n",
    "\n",
    "function(emb::Embeddings)(x::Array)\n",
    "    return emb(x) .* sqrt(d_model.d_model)\n",
    "end\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "function init_pe(max_len::Int, d_model::Int)\n",
    "    pe=zeros(max_len,d_model)\n",
    "    pos = unsqueeze(Array(range(0,length=max_len,step=1)),2)\n",
    "    div_term = exp.(Array(range(0,length=Int(d_model/2),step=2)).*-(log(10000.0) / d_model))\n",
    "    c=pos.* transpose(div_term)\n",
    "    pe[:, range(2,length=Int(d_model/2),step=2)] = cos.(c)\n",
    "    pe[:, range(1,length=Int(d_model/2),step=2)] = sin.(c)\n",
    "    #pe =unsqueeze(pe,1)\n",
    "    return pe\n",
    "    \n",
    "end\n",
    "struct PositionalEncoding\n",
    "     d_model::Int\n",
    "     dropout::Dropout\n",
    "     max_len::Int\n",
    "     pe::Array\n",
    "    PositionalEncoding(d_model::Int,p_dropout::Float64,max_len::Int) = new(d_model,Dropout(p_dropout),max_len,init_pe(max_len,d_model))\n",
    "end\n",
    "\n",
    "function (pe::PositionalEncoding)(x::Array)\n",
    "    return dropout(x.+pe.pe[:,size(x)[2]])\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "function make_model(src_vocab::Int, tgt_vocab::Int, N::Int, \n",
    "               d_model::Int, d_ff::Int, h::Int, dropout::Float64)\n",
    "    \"Helper: Construct a model from hyperparameters.\"\n",
    "    c = deepcopy\n",
    "    attn = MultiHeadedAttention(d_model,h,dropout)\n",
    "    ff = PositionwiseFeedForward(d_model, d_ff, dropout)\n",
    "    position = PositionalEncoding(d_model, dropout,5000)\n",
    "    model = EncoderDecoder(\n",
    "        Encoder(EncoderLayer(d_model, c(attn), c(ff), dropout), N),\n",
    "        Decoder(DecoderLayer(d_model, c(attn), c(attn), \n",
    "                             c(ff), dropout), N),\n",
    "        SourceEmbedding(Chain(Embeddings(d_model, src_vocab), c(position))),\n",
    "        TargetEmbedding(Chain(Embeddings(d_model, tgt_vocab), c(position))),\n",
    "        Generator(d_model, tgt_vocab))\n",
    "    \n",
    "   \n",
    "    # Initialize parameters with Glorot / fan_avg.\n",
    "\n",
    "    return model\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp_model = make_model(10, 10, 2,512,2048,8,0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "function make_std_mask(tgt::Array, pad::Int)\n",
    "        \"Create a mask to hide padding and future words.\"\n",
    "        tgt_mask = (tgt .!= pad)\n",
    "        tgt_mask = unsqueeze(tgt_mask,Array(1:size(tgt_mask)[2])[end])\n",
    "        tgt_mask = tgt_mask .& (subsequent_mask(size(tgt)[end]))\n",
    "        return tgt_mask\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "struct Batch\n",
    "    src::Array\n",
    "    src_mask::Array\n",
    "    trg::Union{Array,Nothing}\n",
    "    trg_mask::Union{Array,Nothing}\n",
    "    ntokens::Int\n",
    "    \n",
    "    function Batch(src::Array, trg_i::Union{Array,Nothing}, pad::Int)\n",
    "    src_mask = (src .!= pad)\n",
    "    src_mask = unsqueeze(src_mask,Array(1:size(src_mask)[2])[end-1])\n",
    "    trg_mask = nothing\n",
    "    ntokens = 0\n",
    "    if trg_i != nothing\n",
    "            trg = trg_i[:, begin:end-1]\n",
    "            trg_y = trg_i[:,2:end]\n",
    "            trg_mask = make_std_mask(trg, pad)\n",
    "            ntokens = sum(trg_y .!= pad)\n",
    "    end\n",
    "    new(src,src_mask,trg,trg_mask,ntokens)\n",
    "end\n",
    "        \n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "function run_epoch!(loss, ps, data, opt)\n",
    "  ps = Flux.params(model)\n",
    " \n",
    "  for batch in batches\n",
    "    # back is a method that computes the product of the gradient so far with its argument.\n",
    "    train_loss, back = Zygote.pullback(() -> loss_compute(model, batch,loss), ps)\n",
    "    # Insert whatever code you want here that needs training_loss, e.g. logging.\n",
    "    # logging_callback(training_loss)\n",
    "    # Apply back() to the correct type of 1.0 to get the gradient of loss.\n",
    "    gs = back(one(train_loss))\n",
    "    # Insert what ever code you want here that needs gradient.\n",
    "    # E.g. logging with TensorBoardLogger.jl as histogram so you can see if it is becoming huge.\n",
    "    update!(opt, ps, gs)\n",
    "    # Here you might like to check validation set accuracy, and break out to do early stopping.\n",
    "  end\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "mutable struct LabelSmoothing\n",
    "    size::Int\n",
    "    criterion::Function\n",
    "    padding_idx::Int\n",
    "    confidence::Float64\n",
    "    smoothing::Float64\n",
    "    true_dist::Union{Array,Nothing}\n",
    "    LabelSmoothing(size::Int,padding_idx::Int,smoothing::Float64) = new(size,StatsBase.kldivergence,padding_idx,1-smoothing,smoothing)\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "@assert (macro with 1 method)"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "macro assert(ex)\n",
    "    return :( $ex ? nothing : throw(AssertionError($(string(ex)))) )\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "scatter_dim! (generic function with 1 method)"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function scatter_dim!(x::Array,target::Array,value::Float64)\n",
    "    for i in 1:size(target)[1]\n",
    "        index=target[i]\n",
    "      \n",
    "        x[i,index]=value\n",
    "    end\n",
    "    return x\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "function (ls::LabelSmoothing)(x::Array,target::Array)\n",
    "    \n",
    "    @assert size(x)[2] == ls.size\n",
    "    true_dist=fill(ls.smoothing / (ls.size - 2), size(x))\n",
    "    true_dist=scatter_dim!(true_dist,target,ls.confidence)\n",
    "    true_dist[:,ls.padding_idx].=0\n",
    "    mask = findall(x->x!=0, target.==ls.padding_idx)\n",
    "    \n",
    "    if size(mask)[1] > 0\n",
    "        true_dist[mask,:].=0\n",
    "    end\n",
    "    ls.true_dist = true_dist\n",
    "    return ls.criterion(x,true_dist)\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mutable struct LossCompute\n",
    " generator::Generator\n",
    " criterion::LabelSmoothing\n",
    " LossCompute(generator::Generator,criterion::LabelSmoothing) = new(generator,criterion)\n",
    "end\n",
    "\n",
    "function(slc::LossCompute)(x::Array,y::Array,norm::Int)\n",
    " x = slc.generator(x)\n",
    " loss = slc.criterion(x,y) / norm\n",
    " return loss\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "function loss_function(model,batch,slc)\n",
    "    out = model(batch.src, batch.trg, batch.src_mask, batch.trg_mask)\n",
    "    loss = slc(out, batch.trg_y, batch.ntokens)\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the simple copy task.\n",
    "V = 11\n",
    "criterion = LabelSmoothing(V, 0, 0.0)\n",
    "model =  make_model(V, V, 2,512,2048,8,0.1) \n",
    "model_opt = Optimiser(ExpDecay(), ADAM())\n",
    "no_of_epoch=10\n",
    "slc =  LossCompute(model.generator,criterion)\n",
    "\n",
    "\n",
    "data = data_gen(V, 30, 20)\n",
    "\n",
    "for epoch in 1:no_of_epoch\n",
    "    run_epoch!(slc,data,model,model_opt)\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def greedy_decode(model, src, src_mask, max_len, start_symbol):\n",
    "    memory = model.encode(src, src_mask)\n",
    "    ys = torch.ones(1, 1).fill_(start_symbol).type_as(src.data)\n",
    "    for i in range(max_len-1):\n",
    "        out = model.decode(memory, src_mask, \n",
    "                           Variable(ys), \n",
    "                           Variable(subsequent_mask(ys.size(1))\n",
    "                                    .type_as(src.data)))\n",
    "        prob = model.generator(out[:, -1])\n",
    "        _, next_word = torch.max(prob, dim = 1)\n",
    "        next_word = next_word.data[0]\n",
    "        ys = torch.cat([ys, \n",
    "                        torch.ones(1, 1).type_as(src.data).fill_(next_word)], dim=1)\n",
    "    return ys\n",
    "\n",
    "model.eval()\n",
    "src = Variable(torch.LongTensor([[1,2,3,4,5,6,7,8,9,10]]) )\n",
    "src_mask = Variable(torch.ones(1, 1, 10) )\n",
    "print(greedy_decode(model, src, src_mask, max_len=10, start_symbol=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "global max_src_in_batch, max_tgt_in_batch\n",
    "def batch_size_fn(new, count, sofar):\n",
    "    \"Keep augmenting batch and calculate total number of tokens + padding.\"\n",
    "    global max_src_in_batch, max_tgt_in_batch\n",
    "    if count == 1:\n",
    "        max_src_in_batch = 0\n",
    "        max_tgt_in_batch = 0\n",
    "    max_src_in_batch = max(max_src_in_batch,  len(new.src))\n",
    "    max_tgt_in_batch = max(max_tgt_in_batch,  len(new.trg) + 2)\n",
    "    src_elements = count * max_src_in_batch\n",
    "    tgt_elements = count * max_tgt_in_batch\n",
    "    return max(src_elements, tgt_elements)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "JuliaPro_v1.5.2-1 1.5.2",
   "language": "julia",
   "name": "juliapro_v1.5.2-1-1.5"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
