{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "using Flux,LinearAlgebra"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "function clone(m::Any, N::Int)\n",
    "    return ([deepcopy(m) for i in 1:N])\n",
    "end\n",
    "\n",
    "mutable struct MultiHeadedAttention\n",
    "        d_model::Int\n",
    "        h::Int\n",
    "        d_k::Int\n",
    "        linears::Array\n",
    "        dropout::Dropout\n",
    "        attn::Union{Array,Nothing}\n",
    "        MultiHeadedAttention(d_model::Int,h::Int,dropout::Float64) = new(\n",
    "        d_model,h,Int(d_model/h),clone(Dense(d_model, d_model), 4),Dropout(dropout),nothing\n",
    "    )\n",
    "\n",
    "end\n",
    "function (mh::MultiHeadedAttention)( query::Array, key::Array, value::Array, mask::Union{Array,Nothing})\n",
    "    \n",
    "    # Same mask applied to all h heads.\n",
    "    if mask != nothing\n",
    "        mask=unsqueeze(mask,2)\n",
    "    end\n",
    "    \n",
    "     nbatches = size(query)[1]\n",
    "    \n",
    "    # 1) Do all the linear projections in batch from d_model => h x d_k \n",
    "     query, key, value = [permutedims(reshape(l(x),nbatches, :, mh.h, mh.d_k),(1,3,2,4))\n",
    "        for (l, x) in zip(mh.linears, (query, key, value))]\n",
    "    \n",
    "    # 2) Apply attention on all the projected vectors in batch. \n",
    "    x, mh.attn = attention(query, key, value, mask, mh.dropout)\n",
    "    \n",
    "    # 3) \"Concat\" using a view and apply a final linear. \n",
    "    x = reshape(permutedims(x,(1,3,2,4)) ,nbatches, :, mh.h * mh.d_k)\n",
    "    \n",
    "    return mh.linears[end](x)\n",
    "end\n",
    "\n",
    "mutable struct PositionwiseFeedForward\n",
    "    w_1::Dense\n",
    "    w_2::Dense\n",
    "    dropout::Dropout\n",
    "    \n",
    "    PositionwiseFeedForward(d_model::Int,d_ff::Int,dropout::Float64) = new(Dense(d_model,d_ff),Dense(d_ff,d_model),Dropout(dropout))\n",
    "\n",
    "end\n",
    "\n",
    "function (pl::PositionwiseFeedForward)(x::Array)\n",
    "    return pl.w_2( pl.dropout( relu(pl.w_1(x) )))\n",
    "end\n",
    "\n",
    "mutable struct Sublayer \n",
    "    size::Int\n",
    "    norm::LayerNorm\n",
    "    dropout::Dropout\n",
    "    Sublayer(size::Int,dropout::Float64) = new(size,LayerNorm(size),Dropout(dropout))\n",
    "end\n",
    "\n",
    "function (sb::Sublayer)(x::Matrix,f::Function)\n",
    "   return x .+ dropout(f(norm(x)))\n",
    "end\n",
    "\n",
    "mutable struct EncoderLayer \n",
    "    size::Int\n",
    "    self_attn::MultiHeadedAttention\n",
    "    feed_forward::PositionwiseFeedForward\n",
    "    sublayer::Array\n",
    "    EncoderLayer(size::Int,\n",
    "        self_attn::MultiHeadedAttention,\n",
    "        feed_forward::PositionwiseFeedForward,\n",
    "        dropout::Float64) = new(size,self_attn,feed_forward,clone(Sublayer(size,dropout),2))\n",
    "end\n",
    "\n",
    "function (el::EncoderLayer)(x::Matrix,src_mask::Matrix)\n",
    "    x = el.sublayer[1](x, (x,src_mask)->el.self_attn(x, x, x, src_mask))\n",
    "    return el.sublayer[2](x, el.feed_forward)\n",
    "end\n",
    "\n",
    "mutable struct Encoder \n",
    "    layers::Array\n",
    "    norm::LayerNorm\n",
    "    Encoder(layer::EncoderLayer,N::Int) = new(clone(layer,N),LayerNorm(layer.size))     \n",
    "end\n",
    "\n",
    "function (en::Encoder)(x::Matrix,src_mask::Matrix)\n",
    "      for layer in en.layers\n",
    "            x = layer(x, src_mask)\n",
    "      end\n",
    "      return self.norm(x)\n",
    "end\n",
    "\n",
    "mutable struct DecoderLayer\n",
    "    size::Int\n",
    "    self_attn::MultiHeadedAttention\n",
    "    src_attn::MultiHeadedAttention\n",
    "    feed_forward::PositionwiseFeedForward\n",
    "    sublayer::Array\n",
    "    \n",
    "    DecoderLayer(size::Int,\n",
    "        self_attn::MultiHeadedAttention,\n",
    "        src_attn::MultiHeadedAttention,\n",
    "        feed_forward::PositionwiseFeedForward,\n",
    "        dropout::Float64) = new(size,self_attn,src_attn,feed_forward,clone(Sublayer(size,dropout),3))\n",
    "end\n",
    "\n",
    "function (dl::DecoderLayer)(x::Matrix,memory::Matrix,src_mask::Matrix,tgt_mask::Matrix)\n",
    "    x = dl.sublayer[1](x, (x,tgt_mask)->dl.self_attn(x, x, x, tgt_mask))\n",
    "    x = dl.sublayer[2](x, (x,src_mask,memory)->dl.src_attn(x, memory, memory, src_mask))\n",
    "    return dl.sublayer[3](x, el.feed_forward)\n",
    "end\n",
    "\n",
    "\n",
    "mutable struct Decoder \n",
    "    layers::Array\n",
    "    norm::LayerNorm\n",
    "    Decoder(layer::DecoderLayer,N::Int) = new(clone(layer,N),LayerNorm(layer.size))  \n",
    "end\n",
    "\n",
    "function (dc::Decoder)(x::Matrix,memory::Matrix,src_mask::Matrix,tgt_mask::Matrix)\n",
    "      for layer in dc.layers\n",
    "            x = layer(x, memory,src_mask,tgt_mask)\n",
    "      end\n",
    "      return self.norm(x)\n",
    "end\n",
    "\n",
    "\n",
    "function subsequent_mask(size::Int)\n",
    "    \"Mask out subsequent positions.\"\n",
    "    return  Array(LowerTriangular(ones(size,size)).==1.0)\n",
    "end\n",
    "\n",
    "function fill_mask!(a::Array,mask::Array,esp=-1e9)\n",
    "    a[.~mask].=esp\n",
    "    return a\n",
    "end\n",
    "\n",
    "function swap_last_2_dimesions!(a::Array)\n",
    "    #swap last 2 dimensions\n",
    "    dimesions=Array(1:length(size(a)))\n",
    "    swp_d_1=dimesions[end]\n",
    "    swp_d_2=dimesions[end-1]\n",
    "    dimesions=append!(dimesions[1:end-2],[swp_d_1,swp_d_2])\n",
    "    return permutedims(a,dimesions)\n",
    "end\n",
    "\n",
    "mutable struct SourceEmbedding\n",
    "    m::Chain\n",
    "end\n",
    "function (se::SourceEmbedding)(x::Array)\n",
    "    return m(x)\n",
    "end\n",
    "\n",
    "mutable struct TargetEmbedding\n",
    "    m::Chain\n",
    "end\n",
    "\n",
    "function (se::TargetEmbedding)(x::Array)\n",
    "    return m(x)\n",
    "end\n",
    "\n",
    "struct Generator\n",
    "    m::Chain    \n",
    "    Generator(d_model::Int,vocab::Int)=new(Chain(Dense(d_model,d_model),logsoftmax))\n",
    "end\n",
    "\n",
    "function (g::Generator)(x::Array)\n",
    "    return g(x)\n",
    "end\n",
    "\n",
    "mutable struct EncoderDecoder\n",
    "    encoder::Encoder\n",
    "    decoder::Decoder\n",
    "    src_embed::SourceEmbedding\n",
    "    tgt_embed::TargetEmbedding\n",
    "    generator::Generator\n",
    "    \n",
    "    EncoderDecoder(\n",
    "    encoder::Encoder,\n",
    "    decoder::Decoder,\n",
    "    src_embed::SourceEmbedding,\n",
    "    tgt_embed::TargetEmbedding,\n",
    "    generator::Generator) =  new(encoder,decoder,src_embed,tgt_embed,generator)\n",
    "end\n",
    "\n",
    "function (ed::EncoderDecoder)(src::Matrix, tgt::Matrix, src_mask::Matrix, tgt_mask::Matrix)\n",
    "    return ed.decoder(ed.tgt_embed(tgt),ed.encoder(ed.src_embed(src),src_mask),src_mask,tgt_mask)\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "attention (generic function with 1 method)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function attention(query, key, value, mask::Union{Array,Nothing}, dropout::Union{Array,Nothing})\n",
    "    \"Compute 'Scaled Dot Product Attention'\"\n",
    "    d_k = size(query)[end]\n",
    "    scores = (query * swap_last_2_dimesions!(key))./math.sqrt(d_k)\n",
    "    if mask != nothing\n",
    "        scores = masked_fill!(scores,mask)\n",
    "    end\n",
    "    p_attn = softmax(scores, dim = length(size(scores)))\n",
    "    \n",
    "    if dropout != nothing\n",
    "        p_attn = dropout(p_attn)\n",
    "    end\n",
    "    return p_attn*value, p_attn\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "unsqueeze (generic function with 1 method)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#unsqueeze\n",
    "unsqueeze(xs, dim) = reshape(xs, (size(xs)[1:dim-1]..., 1, size(xs)[dim:end]...))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "mutable struct EmbeddingLayer\n",
    "   W\n",
    "   EmbeddingLayer(mf, vs) = new(Flux.glorot_normal(mf, vs))\n",
    "end\n",
    "\n",
    "(m::EmbeddingLayer)(x) = m.W * Flux.onehotbatch(reshape(x, pad_size*N), 0:vocab_size-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "mutable struct Embeddings\n",
    "\n",
    "        lut::EmbeddingLayer\n",
    "        d_model::Int\n",
    "\n",
    "    Embeddings(d_model::Int, vocab::Int)=new(EmbeddingLayer(d_model,vocab),d_model)\n",
    "    \n",
    "end\n",
    "\n",
    "function(emb::Embeddings)(x::Array)\n",
    "    return emb(x) .* sqrt(d_model.d_model)\n",
    "end\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "function init_pe(max_len::Int, d_model::Int)\n",
    "    pe=zeros(max_len,d_model)\n",
    "    pos = unsqueeze(Array(range(0,length=max_len,step=1)),2)\n",
    "    div_term = exp.(Array(range(0,length=Int(d_model/2),step=2)).*-(log(10000.0) / d_model))\n",
    "    c=pos.* transpose(div_term)\n",
    "    pe[:, range(2,length=Int(d_model/2),step=2)] = cos.(c)\n",
    "    pe[:, range(1,length=Int(d_model/2),step=2)] = sin.(c)\n",
    "    #pe =unsqueeze(pe,1)\n",
    "    return pe\n",
    "    \n",
    "end\n",
    "struct PositionalEncoding\n",
    "     d_model::Int\n",
    "     dropout::Dropout\n",
    "     max_len::Int\n",
    "     pe::Array\n",
    "    PositionalEncoding(d_model::Int,p_dropout::Float64,max_len::Int) = new(d_model,Dropout(p_dropout),max_len,init_pe(max_len,d_model))\n",
    "end\n",
    "\n",
    "function (pe::PositionalEncoding)(x::Array)\n",
    "    return dropout(x.+pe.pe[:,size(x)[2]])\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "make_model (generic function with 1 method)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function make_model(src_vocab::Int, tgt_vocab::Int, N::Int, \n",
    "               d_model::Int, d_ff::Int, h::Int, dropout::Float64)\n",
    "    \"Helper: Construct a model from hyperparameters.\"\n",
    "    c = deepcopy\n",
    "    attn = MultiHeadedAttention(d_model,h,dropout)\n",
    "    ff = PositionwiseFeedForward(d_model, d_ff, dropout)\n",
    "    position = PositionalEncoding(d_model, dropout,5000)\n",
    "    model = EncoderDecoder(\n",
    "        Encoder(EncoderLayer(d_model, c(attn), c(ff), dropout), N),\n",
    "        Decoder(DecoderLayer(d_model, c(attn), c(attn), \n",
    "                             c(ff), dropout), N),\n",
    "        SourceEmbedding(Chain(Embeddings(d_model, src_vocab), c(position))),\n",
    "        TargetEmbedding(Chain(Embeddings(d_model, tgt_vocab), c(position))),\n",
    "        Generator(d_model, tgt_vocab))\n",
    "    \n",
    "   \n",
    "    # Initialize parameters with Glorot / fan_avg.\n",
    "\n",
    "    return model\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "EncoderDecoder(Encoder(EncoderLayer[EncoderLayer(512, MultiHeadedAttention(512, 8, 64, Dense{typeof(identity),Array{Float32,2},Array{Float32,1}}[Dense(512, 512), Dense(512, 512), Dense(512, 512), Dense(512, 512)], Dropout(0.1), nothing), PositionwiseFeedForward(Dense(512, 2048), Dense(2048, 512), Dropout(0.1)), Sublayer[Sublayer(512, LayerNorm(512), Dropout(0.1)), Sublayer(512, LayerNorm(512), Dropout(0.1))]), EncoderLayer(512, MultiHeadedAttention(512, 8, 64, Dense{typeof(identity),Array{Float32,2},Array{Float32,1}}[Dense(512, 512), Dense(512, 512), Dense(512, 512), Dense(512, 512)], Dropout(0.1), nothing), PositionwiseFeedForward(Dense(512, 2048), Dense(2048, 512), Dropout(0.1)), Sublayer[Sublayer(512, LayerNorm(512), Dropout(0.1)), Sublayer(512, LayerNorm(512), Dropout(0.1))])], LayerNorm(512)), Decoder(DecoderLayer[DecoderLayer(512, MultiHeadedAttention(512, 8, 64, Dense{typeof(identity),Array{Float32,2},Array{Float32,1}}[Dense(512, 512), Dense(512, 512), Dense(512, 512), Dense(512, 512)], Dropout(0.1), nothing), MultiHeadedAttention(512, 8, 64, Dense{typeof(identity),Array{Float32,2},Array{Float32,1}}[Dense(512, 512), Dense(512, 512), Dense(512, 512), Dense(512, 512)], Dropout(0.1), nothing), PositionwiseFeedForward(Dense(512, 2048), Dense(2048, 512), Dropout(0.1)), Sublayer[Sublayer(512, LayerNorm(512), Dropout(0.1)), Sublayer(512, LayerNorm(512), Dropout(0.1)), Sublayer(512, LayerNorm(512), Dropout(0.1))]), DecoderLayer(512, MultiHeadedAttention(512, 8, 64, Dense{typeof(identity),Array{Float32,2},Array{Float32,1}}[Dense(512, 512), Dense(512, 512), Dense(512, 512), Dense(512, 512)], Dropout(0.1), nothing), MultiHeadedAttention(512, 8, 64, Dense{typeof(identity),Array{Float32,2},Array{Float32,1}}[Dense(512, 512), Dense(512, 512), Dense(512, 512), Dense(512, 512)], Dropout(0.1), nothing), PositionwiseFeedForward(Dense(512, 2048), Dense(2048, 512), Dropout(0.1)), Sublayer[Sublayer(512, LayerNorm(512), Dropout(0.1)), Sublayer(512, LayerNorm(512), Dropout(0.1)), Sublayer(512, LayerNorm(512), Dropout(0.1))])], LayerNorm(512)), SourceEmbedding(Chain(Embeddings(EmbeddingLayer(Float32[-0.039719515 0.055074584 … -0.077630185 0.059811603; 0.050916303 -0.036274012 … 0.07471505 -0.020478036; … ; 0.02409464 0.03233741 … -0.0646401 -0.075212024; -0.1297028 -0.048095673 … 0.08341045 -0.1252583]), 512), PositionalEncoding(512, Dropout(0.1), 5000, [0.0 1.0 … 0.0 1.0; 0.8414709848078965 0.5403023058681398 … 0.0001036632926581074 0.9999999946269609; … ; 0.27049952435615754 -0.962720108506669 … 0.4952383239309383 0.868757159688526; -0.6639495210536048 -0.7477773956818223 … 0.4953283794976971 0.8687058169853505]))), TargetEmbedding(Chain(Embeddings(EmbeddingLayer(Float32[0.01979229 -0.08837088 … 0.03180001 0.0147010125; 0.012276442 0.022818232 … -0.06525269 0.06693991; … ; 0.003247627 -0.15662973 … -0.07433067 0.14330873; 0.20744316 -0.06864243 … 0.009955732 -0.07554486]), 512), PositionalEncoding(512, Dropout(0.1), 5000, [0.0 1.0 … 0.0 1.0; 0.8414709848078965 0.5403023058681398 … 0.0001036632926581074 0.9999999946269609; … ; 0.27049952435615754 -0.962720108506669 … 0.4952383239309383 0.868757159688526; -0.6639495210536048 -0.7477773956818223 … 0.4953283794976971 0.8687058169853505]))), Generator(Chain(Dense(512, 512), logsoftmax)))"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tmp_model = make_model(10, 10, 2,512,2048,8,0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "JuliaPro_v1.5.2-1 1.5.2",
   "language": "julia",
   "name": "juliapro_v1.5.2-1-1.5"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
