{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "┌ Info: Precompiling StatsBase [2913bbd2-ae8a-5f71-8c99-4fb6c76f3a91]\n",
      "└ @ Base loading.jl:1278\n"
     ]
    }
   ],
   "source": [
    "using Flux,LinearAlgebra,Distances,StatsBase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "function clone(m::Any, N::Int)\n",
    "    return ([deepcopy(m) for i in 1:N])\n",
    "end\n",
    "\n",
    "mutable struct MultiHeadedAttention\n",
    "        d_model::Int\n",
    "        h::Int\n",
    "        d_k::Int\n",
    "        linears::Array\n",
    "        dropout::Dropout\n",
    "        attn::Union{Array,Nothing}\n",
    "        MultiHeadedAttention(d_model::Int,h::Int,dropout::Float64) = new(\n",
    "        d_model,h,Int(d_model/h),clone(Dense(d_model, d_model), 4),Dropout(dropout),nothing\n",
    "    )\n",
    "\n",
    "end\n",
    "function (mh::MultiHeadedAttention)( query::Array, key::Array, value::Array, mask::Union{Array,Nothing})\n",
    "    \n",
    "    # Same mask applied to all h heads.\n",
    "    if mask != nothing\n",
    "        mask=unsqueeze(mask,2)\n",
    "    end\n",
    "    \n",
    "     nbatches = size(query)[1]\n",
    "    \n",
    "    # 1) Do all the linear projections in batch from d_model => h x d_k \n",
    "     query, key, value = [permutedims(reshape(l(x),nbatches, :, mh.h, mh.d_k),(1,3,2,4))\n",
    "        for (l, x) in zip(mh.linears, (query, key, value))]\n",
    "    \n",
    "    # 2) Apply attention on all the projected vectors in batch. \n",
    "    x, mh.attn = attention(query, key, value, mask, mh.dropout)\n",
    "    \n",
    "    # 3) \"Concat\" using a view and apply a final linear. \n",
    "    x = reshape(permutedims(x,(1,3,2,4)) ,nbatches, :, mh.h * mh.d_k)\n",
    "    \n",
    "    return mh.linears[end](x)\n",
    "end\n",
    "\n",
    "mutable struct PositionwiseFeedForward\n",
    "    w_1::Dense\n",
    "    w_2::Dense\n",
    "    dropout::Dropout\n",
    "    \n",
    "    PositionwiseFeedForward(d_model::Int,d_ff::Int,dropout::Float64) = new(Dense(d_model,d_ff),Dense(d_ff,d_model),Dropout(dropout))\n",
    "\n",
    "end\n",
    "\n",
    "function (pl::PositionwiseFeedForward)(x::Array)\n",
    "    return pl.w_2( pl.dropout( relu(pl.w_1(x) )))\n",
    "end\n",
    "\n",
    "mutable struct Sublayer \n",
    "    size::Int\n",
    "    norm::LayerNorm\n",
    "    dropout::Dropout\n",
    "    Sublayer(size::Int,dropout::Float64) = new(size,LayerNorm(size),Dropout(dropout))\n",
    "end\n",
    "\n",
    "function (sb::Sublayer)(x::Matrix,f::Function)\n",
    "   return x .+ dropout(f(norm(x)))\n",
    "end\n",
    "\n",
    "mutable struct EncoderLayer \n",
    "    size::Int\n",
    "    self_attn::MultiHeadedAttention\n",
    "    feed_forward::PositionwiseFeedForward\n",
    "    sublayer::Array\n",
    "    EncoderLayer(size::Int,\n",
    "        self_attn::MultiHeadedAttention,\n",
    "        feed_forward::PositionwiseFeedForward,\n",
    "        dropout::Float64) = new(size,self_attn,feed_forward,clone(Sublayer(size,dropout),2))\n",
    "end\n",
    "\n",
    "function (el::EncoderLayer)(x::Matrix,src_mask::Matrix)\n",
    "    x = el.sublayer[1](x, (x,src_mask)->el.self_attn(x, x, x, src_mask))\n",
    "    return el.sublayer[2](x, el.feed_forward)\n",
    "end\n",
    "\n",
    "mutable struct Encoder \n",
    "    layers::Array\n",
    "    norm::LayerNorm\n",
    "    Encoder(layer::EncoderLayer,N::Int) = new(clone(layer,N),LayerNorm(layer.size))     \n",
    "end\n",
    "\n",
    "function (en::Encoder)(x::Matrix,src_mask::Matrix)\n",
    "      for layer in en.layers\n",
    "            x = layer(x, src_mask)\n",
    "      end\n",
    "      return self.norm(x)\n",
    "end\n",
    "\n",
    "mutable struct DecoderLayer\n",
    "    size::Int\n",
    "    self_attn::MultiHeadedAttention\n",
    "    src_attn::MultiHeadedAttention\n",
    "    feed_forward::PositionwiseFeedForward\n",
    "    sublayer::Array\n",
    "    \n",
    "    DecoderLayer(size::Int,\n",
    "        self_attn::MultiHeadedAttention,\n",
    "        src_attn::MultiHeadedAttention,\n",
    "        feed_forward::PositionwiseFeedForward,\n",
    "        dropout::Float64) = new(size,self_attn,src_attn,feed_forward,clone(Sublayer(size,dropout),3))\n",
    "end\n",
    "\n",
    "function (dl::DecoderLayer)(x::Matrix,memory::Matrix,src_mask::Matrix,tgt_mask::Matrix)\n",
    "    x = dl.sublayer[1](x, (x,tgt_mask)->dl.self_attn(x, x, x, tgt_mask))\n",
    "    x = dl.sublayer[2](x, (x,src_mask,memory)->dl.src_attn(x, memory, memory, src_mask))\n",
    "    return dl.sublayer[3](x, el.feed_forward)\n",
    "end\n",
    "\n",
    "\n",
    "mutable struct Decoder \n",
    "    layers::Array\n",
    "    norm::LayerNorm\n",
    "    Decoder(layer::DecoderLayer,N::Int) = new(clone(layer,N),LayerNorm(layer.size))  \n",
    "end\n",
    "\n",
    "function (dc::Decoder)(x::Matrix,memory::Matrix,src_mask::Matrix,tgt_mask::Matrix)\n",
    "      for layer in dc.layers\n",
    "            x = layer(x, memory,src_mask,tgt_mask)\n",
    "      end\n",
    "      return self.norm(x)\n",
    "end\n",
    "\n",
    "\n",
    "function subsequent_mask(size::Int)\n",
    "    \"Mask out subsequent positions.\"\n",
    "    return  unsqueeze(Array(LowerTriangular(ones(size,size)).==1.0),1)\n",
    "end\n",
    "\n",
    "function fill_mask!(a::Array,mask::Array,esp=-1e9)\n",
    "    a[.~mask].=esp\n",
    "    return a\n",
    "end\n",
    "\n",
    "function swap_last_2_dimesions!(a::Array)\n",
    "    #swap last 2 dimensions\n",
    "    dimesions=Array(1:length(size(a)))\n",
    "    swp_d_1=dimesions[end]\n",
    "    swp_d_2=dimesions[end-1]\n",
    "    dimesions=append!(dimesions[1:end-2],[swp_d_1,swp_d_2])\n",
    "    return permutedims(a,dimesions)\n",
    "end\n",
    "\n",
    "mutable struct SourceEmbedding\n",
    "    m::Chain\n",
    "end\n",
    "function (se::SourceEmbedding)(x::Array)\n",
    "    return m(x)\n",
    "end\n",
    "\n",
    "mutable struct TargetEmbedding\n",
    "    m::Chain\n",
    "end\n",
    "\n",
    "function (se::TargetEmbedding)(x::Array)\n",
    "    return m(x)\n",
    "end\n",
    "\n",
    "struct Generator\n",
    "    m::Chain    \n",
    "    Generator(d_model::Int,vocab::Int)=new(Chain(Dense(d_model,d_model),logsoftmax))\n",
    "end\n",
    "\n",
    "function (g::Generator)(x::Array)\n",
    "    return g(x)\n",
    "end\n",
    "\n",
    "mutable struct EncoderDecoder\n",
    "    encoder::Encoder\n",
    "    decoder::Decoder\n",
    "    src_embed::SourceEmbedding\n",
    "    tgt_embed::TargetEmbedding\n",
    "    generator::Generator\n",
    "    \n",
    "    EncoderDecoder(\n",
    "    encoder::Encoder,\n",
    "    decoder::Decoder,\n",
    "    src_embed::SourceEmbedding,\n",
    "    tgt_embed::TargetEmbedding,\n",
    "    generator::Generator) =  new(encoder,decoder,src_embed,tgt_embed,generator)\n",
    "end\n",
    "\n",
    "function (ed::EncoderDecoder)(src::Matrix, tgt::Matrix, src_mask::Matrix, tgt_mask::Matrix)\n",
    "    return ed.decoder(ed.tgt_embed(tgt),ed.encoder(ed.src_embed(src),src_mask),src_mask,tgt_mask)\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "function attention(query, key, value, mask::Union{Array,Nothing}, dropout::Union{Array,Nothing})\n",
    "    \"Compute 'Scaled Dot Product Attention'\"\n",
    "    d_k = size(query)[end]\n",
    "    scores = (query * swap_last_2_dimesions!(key))./math.sqrt(d_k)\n",
    "    if mask != nothing\n",
    "        scores = masked_fill!(scores,mask)\n",
    "    end\n",
    "    p_attn = softmax(scores, dim = length(size(scores)))\n",
    "    \n",
    "    if dropout != nothing\n",
    "        p_attn = dropout(p_attn)\n",
    "    end\n",
    "    return p_attn*value, p_attn\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#unsqueeze\n",
    "unsqueeze(xs, dim) = reshape(xs, (size(xs)[1:dim-1]..., 1, size(xs)[dim:end]...))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mutable struct EmbeddingLayer\n",
    "   W\n",
    "   EmbeddingLayer(mf, vs) = new(Flux.glorot_normal(mf, vs))\n",
    "end\n",
    "\n",
    "(m::EmbeddingLayer)(x) = m.W * Flux.onehotbatch(reshape(x, pad_size*N), 0:vocab_size-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mutable struct Embeddings\n",
    "\n",
    "        lut::EmbeddingLayer\n",
    "        d_model::Int\n",
    "\n",
    "    Embeddings(d_model::Int, vocab::Int)=new(EmbeddingLayer(d_model,vocab),d_model)\n",
    "    \n",
    "end\n",
    "\n",
    "function(emb::Embeddings)(x::Array)\n",
    "    return emb(x) .* sqrt(d_model.d_model)\n",
    "end\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "function init_pe(max_len::Int, d_model::Int)\n",
    "    pe=zeros(max_len,d_model)\n",
    "    pos = unsqueeze(Array(range(0,length=max_len,step=1)),2)\n",
    "    div_term = exp.(Array(range(0,length=Int(d_model/2),step=2)).*-(log(10000.0) / d_model))\n",
    "    c=pos.* transpose(div_term)\n",
    "    pe[:, range(2,length=Int(d_model/2),step=2)] = cos.(c)\n",
    "    pe[:, range(1,length=Int(d_model/2),step=2)] = sin.(c)\n",
    "    #pe =unsqueeze(pe,1)\n",
    "    return pe\n",
    "    \n",
    "end\n",
    "struct PositionalEncoding\n",
    "     d_model::Int\n",
    "     dropout::Dropout\n",
    "     max_len::Int\n",
    "     pe::Array\n",
    "    PositionalEncoding(d_model::Int,p_dropout::Float64,max_len::Int) = new(d_model,Dropout(p_dropout),max_len,init_pe(max_len,d_model))\n",
    "end\n",
    "\n",
    "function (pe::PositionalEncoding)(x::Array)\n",
    "    return dropout(x.+pe.pe[:,size(x)[2]])\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "function make_model(src_vocab::Int, tgt_vocab::Int, N::Int, \n",
    "               d_model::Int, d_ff::Int, h::Int, dropout::Float64)\n",
    "    \"Helper: Construct a model from hyperparameters.\"\n",
    "    c = deepcopy\n",
    "    attn = MultiHeadedAttention(d_model,h,dropout)\n",
    "    ff = PositionwiseFeedForward(d_model, d_ff, dropout)\n",
    "    position = PositionalEncoding(d_model, dropout,5000)\n",
    "    model = EncoderDecoder(\n",
    "        Encoder(EncoderLayer(d_model, c(attn), c(ff), dropout), N),\n",
    "        Decoder(DecoderLayer(d_model, c(attn), c(attn), \n",
    "                             c(ff), dropout), N),\n",
    "        SourceEmbedding(Chain(Embeddings(d_model, src_vocab), c(position))),\n",
    "        TargetEmbedding(Chain(Embeddings(d_model, tgt_vocab), c(position))),\n",
    "        Generator(d_model, tgt_vocab))\n",
    "    \n",
    "   \n",
    "    # Initialize parameters with Glorot / fan_avg.\n",
    "\n",
    "    return model\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp_model = make_model(10, 10, 2,512,2048,8,0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "function make_std_mask(tgt::Array, pad::Int)\n",
    "        \"Create a mask to hide padding and future words.\"\n",
    "        tgt_mask = (tgt .!= pad)\n",
    "        tgt_mask = unsqueeze(tgt_mask,Array(1:size(tgt_mask)[2])[end])\n",
    "        tgt_mask = tgt_mask .& (subsequent_mask(size(tgt)[end]))\n",
    "        return tgt_mask\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "struct Batch\n",
    "    src::Array\n",
    "    src_mask::Array\n",
    "    trg::Union{Array,Nothing}\n",
    "    trg_mask::Union{Array,Nothing}\n",
    "    ntokens::Int\n",
    "    \n",
    "    function Batch(src::Array, trg_i::Union{Array,Nothing}, pad::Int)\n",
    "    src_mask = (src .!= pad)\n",
    "    src_mask = unsqueeze(src_mask,Array(1:size(src_mask)[2])[end-1])\n",
    "    trg_mask = nothing\n",
    "    ntokens = 0\n",
    "    if trg_i != nothing\n",
    "            trg = trg_i[:, begin:end-1]\n",
    "            trg_y = trg_i[:,2:end]\n",
    "            trg_mask = make_std_mask(trg, pad)\n",
    "            ntokens = sum(trg_y .!= pad)\n",
    "    end\n",
    "    new(src,src_mask,trg,trg_mask,ntokens)\n",
    "end\n",
    "        \n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Batch(src,src,0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "opt = Optimiser(ExpDecay(), ADAM())\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_epoch(data_iter, model, loss_compute):\n",
    "    \"Standard Training and Logging Function\"\n",
    "    start = time.time()\n",
    "    total_tokens = 0\n",
    "    total_loss = 0\n",
    "    tokens = 0\n",
    "    for i, batch in enumerate(data_iter):\n",
    "        out = model.forward(batch.src, batch.trg, \n",
    "                            batch.src_mask, batch.trg_mask)\n",
    "        loss = loss_compute(out, batch.trg_y, batch.ntokens)\n",
    "        total_loss += loss\n",
    "        total_tokens += batch.ntokens\n",
    "        tokens += batch.ntokens\n",
    "        if i % 50 == 1:\n",
    "            elapsed = time.time() - start\n",
    "            print(\"Epoch Step: %d Loss: %f Tokens per Sec: %f\" %\n",
    "                    (i, loss / batch.ntokens, tokens / elapsed))\n",
    "            start = time.time()\n",
    "            tokens = 0\n",
    "    return total_loss / total_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "function my_custom_train!(loss, ps, data, opt)\n",
    "  ps = Params(ps)\n",
    "  for d in data\n",
    "    # back is a method that computes the product of the gradient so far with its argument.\n",
    "    train_loss, back = Zygote.pullback(() -> loss(d...), ps)\n",
    "    # Insert whatever code you want here that needs training_loss, e.g. logging.\n",
    "    # logging_callback(training_loss)\n",
    "    # Apply back() to the correct type of 1.0 to get the gradient of loss.\n",
    "    gs = back(one(train_loss))\n",
    "    # Insert what ever code you want here that needs gradient.\n",
    "    # E.g. logging with TensorBoardLogger.jl as histogram so you can see if it is becoming huge.\n",
    "    update!(opt, ps, gs)\n",
    "    # Here you might like to check validation set accuracy, and break out to do early stopping.\n",
    "  end\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "typeof(kl_divergence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LabelSmoothing(nn.Module):\n",
    "    \"Implement label smoothing.\"\n",
    "    def __init__(self, size, padding_idx, smoothing=0.0):\n",
    "        super(LabelSmoothing, self).__init__()\n",
    "        self.criterion = nn.KLDivLoss(size_average=False)\n",
    "        self.padding_idx = padding_idx\n",
    "        self.confidence = 1.0 - smoothing\n",
    "        self.smoothing = smoothing\n",
    "        self.size = size\n",
    "        self.true_dist = None\n",
    "        \n",
    "    def forward(self, x, target):\n",
    "        assert x.size(1) == self.size\n",
    "        true_dist = x.data.clone()\n",
    "        true_dist.fill_(self.smoothing / (self.size - 2))\n",
    "        true_dist.scatter_(1, target.data.unsqueeze(1), self.confidence)\n",
    "        true_dist[:, self.padding_idx] = 0\n",
    "        mask = torch.nonzero(target.data == self.padding_idx)\n",
    "        if mask.dim() > 0:\n",
    "            true_dist.index_fill_(0, mask.squeeze(), 0.0)\n",
    "        self.true_dist = true_dist\n",
    "        return self.criterion(x, Variable(true_dist, requires_grad=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "mutable struct LabelSmoothing\n",
    "    size::Int\n",
    "    criterion::Function\n",
    "    padding_idx::Int\n",
    "    confidence::Float64\n",
    "    smoothing::Float64\n",
    "    true_dist::Union{Array,Nothing}\n",
    "    LabelSmoothing(size::Int,padding_idx::Int,smoothing::Float64) = new(size,StatsBase.kldivergence,padding_idx,1-smoothing,smoothing)\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3-element Array{Int64,1}:\n",
       " 3\n",
       " 2\n",
       " 1"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x=[0 0.2 0.7 0.1 0; 0 0.2 0.7 0.1 0 ; 0 0.2 0.7 0.1 0]\n",
    "target=[3, 2, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "@assert (macro with 1 method)"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "macro assert(ex)\n",
    "    return :( $ex ? nothing : throw(AssertionError($(string(ex)))) )\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "scatter_dim! (generic function with 1 method)"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function scatter_dim!(x::Array,target::Array,value::Float64)\n",
    "    for i in 1:size(target)[1]\n",
    "        index=target[i]\n",
    "      \n",
    "        x[i,index]=value\n",
    "    end\n",
    "    return x\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "function (ls::LabelSmoothing)(x::Array,target::Array)\n",
    "    \n",
    "    @assert size(x)[2] == ls.size\n",
    "    true_dist=fill(ls.smoothing / (ls.size - 2), size(x))\n",
    "    true_dist=scatter_dim!(true_dist,target,ls.confidence)\n",
    "    true_dist[:,ls.padding_idx].=0\n",
    "    mask = findall(x->x!=0, target.==ls.padding_idx)\n",
    "    if size(mask)[1] > 0\n",
    "        true_dist[mask,:].=0\n",
    "    end\n",
    "    ls.true_dist = true_dist\n",
    "    return ls.criterion(x,true_dist)\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LabelSmoothing(5, StatsBase.kldivergence, 1, 0.6, 0.4, #undef)"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ls=LabelSmoothing(5, 1, 0.4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Inf"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "v=ls(x,target)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "JuliaPro_v1.5.2-1 1.5.2",
   "language": "julia",
   "name": "juliapro_v1.5.2-1-1.5"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
